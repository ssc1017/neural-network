# neural-network

### 4.5.2 为何要设定损失函数

如果用识别精度作为指标，那么有可能一点细微的参数改变不会改变精确度，并且精确度的变化不是连续的。比如100个样本识别对了32个那么精确度是32%，改变点参数可能还是32%。所以要用损失函数，因为能够通过损失函数的梯度来判断需要如何改变。顺便一提之所以说激活函数不能用阶跃函数而用sigmoid也是如此，因为阶跃函数的导数大部分情况下都是0，只在某个瞬间进行变化。神经网络将无法学习。所以sigmoid斜率不为0的性质才使其成为神经网络的选择。

## 4.3 数值微分

梯度法是梯度的信息决定前进的方向

### 4.3.1 导数

计算机求导数可以通过数值微分的方式进行，数值微分就是通过数值的方式近似求解函数导数，利用微小的差分来求导数就是数值微分numerical differentiation，利用数学式子推导求导数就是解析性analytic。数值微分求导有两个问题，第一个问题是舍入误差，数值太小就直接变成0了，另一个是数值微分毕竟不是导数，即使很接近，所以为了减少误差可以通过计算中心差分的方式。

### 4.3.2 数值微分的例子

### 4.3.3 偏导数

偏导数将某个变量设定成目标变量，然后其它变量设定成固定值。

## 4.4 梯度

之前偏导数是分别计算不同变量的导数，把全部变量的偏导数汇总而成的向量称为梯度。梯度指向各点处函数值减少最多的方向。

### 4.4.1 梯度法

我们要让损失函数减少，就要通过梯度来尽可能寻找函数的最小值，这就是梯度法。函数的极小、最小值以及鞍点saddle point（一个方向是极小值另一个方向是极大值的点），梯度都是0。通过沿梯度不断前进逐渐减小函数值的方法就是梯度法gradient method。严格地说，寻找最小值的梯度法是梯度下降法gradient descent method，寻找最大值的梯度法是梯度上升法gradient ascent method。eta在神经网络中被称为学习率learning rate，决定了一次学习中应该学习多少，在多大程度上更新参数。学习率过大或过小都不能得到很好的结果。学习率这样的参数称为超参数，相对于神经网络中通过训练获得的权重参数，超参数是人工设定的，一般来说超参数可以尝试多个值。

### 4.4.2 神经网络的梯度

神经网络的梯度是指损失函数关于权重参数的梯度

## 4.5 学习算法的实现

步骤1 mini-batch从训练集中随机选取部分数据 步骤2 计算梯度 步骤3 更新参数 步骤4 重复之前的步骤。 因为选取的数据集是随机选择的mini batch数据，所以又称为随机梯度下降法stochastic gradient descent。一般用SGD缩写表示。

# 5 误差反向传播法

之前用数值微分计算了神经网络的权重参数的梯度，但是数值微分比较消耗时间，有个高效计算权重参数的梯度方法就是误差反向传播法。理解误差反向传播有两种方法：基于数学式和基于计算图，基于计算图更加容易理解。

## 5.1 计算图

计算图将计算过程用图形表示出来。

### 5.1.1 用计算图求解

流程是 1.构建计算图 2.在计算图上从左向右计算。这里从左向右计算意味着正向传播。

### 5.1.2 局部计算

计算图的特征是可以通过传递局部计算获得最终结果，局部的意思是与自己相关的某个小范围，无论全局发生了什么都能只根据与自己相关的信息输出接下来的结果。每个节点只关心自己的局部计算，全局计算不需要考虑。
